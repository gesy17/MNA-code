{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from numpy.linalg import cholesky\n",
    "\n",
    "# change it to your file directory\n",
    "model_path ='/data/qit16/Lib/word2vec_twitter_model.bin'\n",
    "source_file='/data/gesy/MNA/WebMD8000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     10,
     32
    ]
   },
   "outputs": [],
   "source": [
    "# open file and get raw data\n",
    "source=[]\n",
    "drug=[]\n",
    "easeofuse=[]\n",
    "effective=[]\n",
    "satisfy=[]\n",
    "text=[]\n",
    "adh=[]\n",
    "disc=[]\n",
    "ADR=[]\n",
    "with open(source_file,encoding='ISO-8859-1') as file:\n",
    "    csv_reader = csv.reader(file) \n",
    "    for row in csv_reader:  \n",
    "        source.append(row[0].lower())\n",
    "        drug.append(row[1])\n",
    "        easeofuse.append(row[2])\n",
    "        effective.append(row[3])\n",
    "        satisfy.append(row[4])\n",
    "        text.append(row[5])\n",
    "        adh.append(row[6])\n",
    "        disc.append(row[7])\n",
    "        ADR.append(row[8])\n",
    "source=source[1:]\n",
    "drug=drug[1:]\n",
    "easeofuse=easeofuse[1:]\n",
    "effective=effective[1:]\n",
    "satisfy=satisfy[1:]\n",
    "text=text[1:]\n",
    "adh=adh[1:]\n",
    "disc=disc[1:]\n",
    "ADR=ADR[1:]\n",
    "labels=np.zeros(len(source))\n",
    "for i in range(len(source)):\n",
    "    if adh[i]=='1' or disc[i]=='1':\n",
    "        labels[i]=1\n",
    "easeofuse=np.array(easeofuse)\n",
    "effective=np.array(effective)\n",
    "satisfy=np.array(satisfy)\n",
    "count=0\n",
    "for d in drug:\n",
    "    if d[0]=='user':\n",
    "        d=d[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    tokenized_texts=[]\n",
    "    for text in texts:  \n",
    "        from nltk.tokenize import TweetTokenizer\n",
    "        tokenizer= TweetTokenizer(strip_handles=True, reduce_len = True, preserve_case=False)\n",
    "        tokenized_text=tokenizer.tokenize(text)\n",
    "        tokenized_texts.append(tokenized_text)\n",
    "    return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generate_indices_embedding(tokenized_text,model_path):\n",
    "    #change default directory to the one containing the module 'word2vecReader'\n",
    "    sys.path.append('/data/qit16/Lib')\n",
    "    from word2vecReader import Word2Vec\n",
    "    \n",
    "    #count apppearing times of words\n",
    "    word_indices_count={'PADDING':[0,99999]}\n",
    "    word_indices={'PADDING':0}\n",
    "    embedding_index={'PADDING':0}\n",
    "    for sentence in tokenized_text:\n",
    "        for word in sentence:\n",
    "            if word not in word_indices_count:\n",
    "                word_indices_count[word]=[len(word_indices_count),1]\n",
    "            else:\n",
    "                word_indices_count[word][1]+=1\n",
    "    for word in word_indices_count.keys():\n",
    "        word_indices[word]=len(word_indices)\n",
    "    total_dict=Word2Vec.load_word2vec_format(model_path, binary=True)\n",
    "    print(\"The vocabulary size is: \"+str(len(total_dict.vocab)))\n",
    "    \n",
    "    #generate new embeddings for out-of-vocabulary words\n",
    "    count=0\n",
    "    for word in word_indices.keys():\n",
    "        if word in total_dict.vocab.keys():\n",
    "            count+=1 \n",
    "        elif word_indices_count[word][1]>1:\n",
    "            count+=1\n",
    "    lister=np.zeros((count,400),dtype='float32')\n",
    "    \n",
    "    for word in word_indices.keys():\n",
    "        if word in total_dict.vocab.keys() and word!='PADDING':\n",
    "            embedding_index[word]=len(embedding_index)\n",
    "            lister[embedding_index[word]]=total_dict.syn0[total_dict.vocab[word].index]\n",
    "    reference=lister[:len(embedding_index)-1]\n",
    "    mu=np.mean(reference, axis=0)\n",
    "    Sigma=np.cov(reference.T)\n",
    "    for word in word_indices_count.keys():\n",
    "        if word not in embedding_index.keys() and word_indices_count[word][1]>1:\n",
    "            embedding_index[word]=len(embedding_index)\n",
    "            lister[embedding_index[word]]=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "   \n",
    "    # generate indices for tokenized text        \n",
    "    text_word_indices=[]\n",
    "    for text in tokenized_text:\n",
    "        text_word_index=[]\n",
    "        for word in text:\n",
    "            if word in embedding_index:\n",
    "                text_word_index.append(embedding_index[word])\n",
    "            else:\n",
    "                text_word_index.append(embedding_index['PADDING'])\n",
    "        text_word_indices.append(text_word_index)\n",
    "   \n",
    "    return text_word_indices,lister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# tokenize the reviews\n",
    "tokenized_text=tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size is: 3039345\n"
     ]
    }
   ],
   "source": [
    "# 'all_train' is the network input for tokenized text, 'lister' is the embedding matrix\n",
    "all_train,lister=generate_indices_embedding(tokenized_text,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     16,
     76,
     112
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# define some modules in the network architecture\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers \n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Optimizer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12        \n",
    "    def call(self, x):\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))    \n",
    "        A = K.softmax(A)\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        # self.init = initializations.get('normal')#keras1.2.2\n",
    "        self.init = initializers.get('normal')\n",
    "\n",
    "        # self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], 1)))\n",
    "        # self.W = self.init((input_shape[-1],))\n",
    "        # self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.tanh(K.dot(x, self.W))\n",
    "        #print(x.shape)\n",
    "        #print(self.W.shape)\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        print(ai.shape)\n",
    "        # weights = ai / K.sum(ai, axis=1).dimshuffle(0, 'x')\n",
    "        weights = ai / K.expand_dims(K.sum(ai, axis=1), 1)\n",
    "        #print('weights', weights.shape)\n",
    "        # weighted_input = x * weights.dimshuffle(0, 1, 'x')\n",
    "        weighted_input = x * weights\n",
    "\n",
    "        # return weighted_input.sum(axis=1)\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., amsgrad=False, **kwargs):\n",
    "        super(Adam, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "myadam = Adam(lr=0.001,amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# split data into training set and testing set\n",
    "indices = np.arange(len(all_train))\n",
    "np.random.shuffle(indices)\n",
    "all_train=pad_sequences(all_train, maxlen=100, dtype='int32', padding='post', truncating='post', value=0)\n",
    "train_sen=all_train[indices[:int(0.8*len(indices))]]\n",
    "train_ease=easeofuse[indices[:int(0.8*len(indices))]]\n",
    "train_effect=effective[indices[:int(0.8*len(indices))]]\n",
    "train_satis=satisfy[indices[:int(0.8*len(indices))]]\n",
    "train_labels=labels[indices[:int(0.8*len(indices))]]\n",
    "\n",
    "val_sen=all_train[indices[int(0.8*len(indices)):]]\n",
    "val_ease=easeofuse[indices[int(0.8*len(indices)):]]\n",
    "val_effect=effective[indices[int(0.8*len(indices)):]]\n",
    "val_satis=satisfy[indices[int(0.8*len(indices)):]]\n",
    "val_labels=labels[indices[int(0.8*len(indices)):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# define the model architecture\n",
    "text_input = Input(shape=(100,), dtype='int32')\n",
    "ease_input= Input(shape=(1,), dtype='int32')\n",
    "effect_input= Input(shape=(1,), dtype='int32')\n",
    "satis_input=Input(shape=(1,), dtype='int32')\n",
    "\n",
    "text_embedding_layer = Embedding(lister.shape[0],400,weights=[lister], trainable=True)\n",
    "ease_embedding_layer = Embedding(5,5,input_length=1,trainable=True)\n",
    "effect_embedding_layer = Embedding(5,5,input_length=1,trainable=True)\n",
    "satis_embedding_layer = Embedding(2,5,input_length=1,trainable=True)\n",
    "\n",
    "embedded_text = text_embedding_layer(text_input)\n",
    "embedded_ease = ease_embedding_layer(ease_input)\n",
    "embedded_effect = effect_embedding_layer(effect_input)\n",
    "embedded_satis = satis_embedding_layer(satis_input)\n",
    "d_embedded_text=Dropout(0.2)(embedded_text)\n",
    "d_embedded_ease= Dropout(0.2)(embedded_ease)\n",
    "d_embedded_effect= Dropout(0.2)(embedded_effect)\n",
    "d_embedded_satis= Dropout(0.2)(embedded_satis)\n",
    "\n",
    "cnn1=Conv1D(padding=\"same\", activation=\"relu\", strides=1, filters=300, kernel_size=3)(d_embedded_text)\n",
    "d_cnn1= Dropout(0.2)(cnn1)\n",
    "p=GlobalMaxPooling1D()(d_cnn1)\n",
    "cp=concatenate([p,Flatten()(d_embedded_ease),Flatten()(d_embedded_effect),Flatten()(d_embedded_satis)],axis=-1)\n",
    "\n",
    "dense= Dense(200, activation='relu')(cp)   \n",
    "final= Dense(2, activation='softmax')(dense)\n",
    "model = Model([text_input,ease_input,effect_input,satis_input], final)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=myadam, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     2,
     16
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6950/6950 [==============================] - 20s 3ms/step - loss: 0.5237 - acc: 0.7386\n",
      "1738/1738 [==============================] - 2s 1ms/step\n",
      "0.8043728423475259\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8323    0.8998    0.8648      1208\n",
      "         1.0     0.7199    0.5868    0.6466       530\n",
      "\n",
      "   micro avg     0.8044    0.8044    0.8044      1738\n",
      "   macro avg     0.7761    0.7433    0.7557      1738\n",
      "weighted avg     0.7980    0.8044    0.7982      1738\n",
      "\n",
      "Epoch 2/10\n",
      "6950/6950 [==============================] - 8s 1ms/step - loss: 0.3737 - acc: 0.8295\n",
      "1738/1738 [==============================] - 1s 586us/step\n",
      "0.8227848101265823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8623    0.8866    0.8743      1208\n",
      "         1.0     0.7238    0.6774    0.6998       530\n",
      "\n",
      "   micro avg     0.8228    0.8228    0.8228      1738\n",
      "   macro avg     0.7931    0.7820    0.7870      1738\n",
      "weighted avg     0.8201    0.8228    0.8211      1738\n",
      "\n",
      "Epoch 3/10\n",
      "6950/6950 [==============================] - 7s 1ms/step - loss: 0.2689 - acc: 0.8847A: 0s - loss: 0.2668\n",
      "1738/1738 [==============================] - 1s 612us/step\n",
      "0.7261219792865362\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9357    0.6507    0.7676      1208\n",
      "         1.0     0.5301    0.8981    0.6667       530\n",
      "\n",
      "   micro avg     0.7261    0.7261    0.7261      1738\n",
      "   macro avg     0.7329    0.7744    0.7171      1738\n",
      "weighted avg     0.8120    0.7261    0.7368      1738\n",
      "\n",
      "Epoch 4/10\n",
      "6950/6950 [==============================] - 7s 1ms/step - loss: 0.1321 - acc: 0.9505\n",
      "1738/1738 [==============================] - 0s 214us/step\n",
      "0.7991944764096662\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8649    0.8427    0.8537      1208\n",
      "         1.0     0.6613    0.7000    0.6801       530\n",
      "\n",
      "   micro avg     0.7992    0.7992    0.7992      1738\n",
      "   macro avg     0.7631    0.7714    0.7669      1738\n",
      "weighted avg     0.8028    0.7992    0.8007      1738\n",
      "\n",
      "Epoch 5/10\n",
      "6950/6950 [==============================] - 7s 957us/step - loss: 0.0420 - acc: 0.9899\n",
      "1738/1738 [==============================] - 1s 404us/step\n",
      "0.7968929804372842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8301    0.8899    0.8590      1208\n",
      "         1.0     0.6998    0.5849    0.6372       530\n",
      "\n",
      "   micro avg     0.7969    0.7969    0.7969      1738\n",
      "   macro avg     0.7649    0.7374    0.7481      1738\n",
      "weighted avg     0.7904    0.7969    0.7913      1738\n",
      "\n",
      "Epoch 6/10\n",
      "6950/6950 [==============================] - 7s 997us/step - loss: 0.0134 - acc: 0.9983\n",
      "1738/1738 [==============================] - 1s 437us/step\n",
      "0.7957422324510932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8636    0.8386    0.8509      1208\n",
      "         1.0     0.6549    0.6981    0.6758       530\n",
      "\n",
      "   micro avg     0.7957    0.7957    0.7957      1738\n",
      "   macro avg     0.7592    0.7683    0.7634      1738\n",
      "weighted avg     0.7999    0.7957    0.7975      1738\n",
      "\n",
      "Epoch 7/10\n",
      "6950/6950 [==============================] - 6s 919us/step - loss: 0.0067 - acc: 0.9991 0s - loss: 0.0068 -\n",
      "1738/1738 [==============================] - 1s 406us/step\n",
      "0.7899884925201381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8551    0.8402    0.8476      1208\n",
      "         1.0     0.6497    0.6755    0.6623       530\n",
      "\n",
      "   micro avg     0.7900    0.7900    0.7900      1738\n",
      "   macro avg     0.7524    0.7579    0.7550      1738\n",
      "weighted avg     0.7925    0.7900    0.7911      1738\n",
      "\n",
      "Epoch 8/10\n",
      "6950/6950 [==============================] - 7s 993us/step - loss: 0.0026 - acc: 0.9999\n",
      "1738/1738 [==============================] - 1s 508us/step\n",
      "0.7784810126582279\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8938    0.7732    0.8291      1208\n",
      "         1.0     0.6046    0.7906    0.6852       530\n",
      "\n",
      "   micro avg     0.7785    0.7785    0.7785      1738\n",
      "   macro avg     0.7492    0.7819    0.7572      1738\n",
      "weighted avg     0.8056    0.7785    0.7852      1738\n",
      "\n",
      "Epoch 9/10\n",
      "6950/6950 [==============================] - 7s 1ms/step - loss: 0.0039 - acc: 0.9996\n",
      "1738/1738 [==============================] - 1s 557us/step\n",
      "0.8009205983889528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8574    0.8560    0.8567      1208\n",
      "         1.0     0.6729    0.6755    0.6742       530\n",
      "\n",
      "   micro avg     0.8009    0.8009    0.8009      1738\n",
      "   macro avg     0.7652    0.7657    0.7654      1738\n",
      "weighted avg     0.8011    0.8009    0.8010      1738\n",
      "\n",
      "Epoch 10/10\n",
      "6950/6950 [==============================] - 7s 1ms/step - loss: 0.0022 - acc: 0.9999\n",
      "1738/1738 [==============================] - 1s 366us/step\n",
      "0.7957422324510932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8642    0.8377    0.8508      1208\n",
      "         1.0     0.6543    0.7000    0.6764       530\n",
      "\n",
      "   micro avg     0.7957    0.7957    0.7957      1738\n",
      "   macro avg     0.7593    0.7689    0.7636      1738\n",
      "weighted avg     0.8002    0.7957    0.7976      1738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_accuracy_score=[]\n",
    "report=[]\n",
    "class scoreHistory(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        predictions = model.predict([val_sen,val_ease,val_effect,val_satis], batch_size=32, verbose=1)\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        true = val_labels\n",
    "        cr = classification_report(true, predictions,digits=4)\n",
    "        acc_score=accuracy_score(true, predictions)\n",
    "        print(acc_score)\n",
    "        print(cr)\n",
    "        report_accuracy_score.append(acc_score)\n",
    "        report.append(cr)\n",
    "scorehistory = scoreHistory()\n",
    "\n",
    "#train the model and test every epoch using the callback function 'scoreHistory'\n",
    "history=model.fit([train_sen,train_ease,train_effect,train_satis],to_categorical(train_labels),batch_size=64,callbacks=[scorehistory],\n",
    "                   epochs=10,shuffle=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
